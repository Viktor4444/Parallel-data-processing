# Parallel-data-processing
## Распаралеливание работы с данными

**Репозиторий для выполнения тествого задания*

[Описание решения](#solve)
### Задача №1:
#### Дано:
Стандартная библиотека Python 3.10+, библиотека генерации данных Faker.
#### Нужно:
Создать программу, которая будет одновременно генерировать данные, изменять их и записывать в файл.

##### Предполагаемая архитектура:
* Класс Генератор данных, который генерирует данные каждые 0.5 сек вне зависимости от других компонентов программы.
* Класс Обработчик данных, который будет читать генерируемые данные, обрабатывать их и записывать в файл с обработанными данными. 
* Класс Отправитель обработанных данных, будет проверять файл с обработанными данными на их наличие, в случае обнаружения данных, удалять их из файла и “отправлять” на воображаемый сервер, с выводом в консоль сообщений об успешной отправке.

**За генерацию фейковых данных, будет отвечать библиотека [`Faker`](https://faker.readthedocs.io/en/stable/index.html). На её основе предполагается генерация полных данных о [пользователях](https://faker.readthedocs.io/en/stable/providers/faker.providers.profile.html).**

Валидация данных будет происходить с условием что нам нужны все данные о пользователях на отрезке возраста [30, 40] лет.
Запись в файл может реализовываться посредством библиотеки [`pickle`](https://docs.python.org/3/library/pickle.html#).

Отправитель данных должен записывать состояние отправки в STDOUT и выводить небольшую информацию о пользователе (ФИО, пол, профессия, возраст в годах (33 года, 30 лет и тд)).

Программа должна запускаться из командной строки, обязательным аргументом будет являться количество изначально генерируемых данных(строк).

#### Результат:

Результат работы программы:

```
python3 main.py - - *named_flag* 3000
```

Сгенерирует 3000 записей о пользователях, из них в консоль с отметкой об успешной отправке на сервер должны вывестись все записи в указанном условии валидации.

# <a name="solve"></a> Описание решения задачи

### Описание архитектуры

Для решения задачи был выбран подход multiprocessing, т.к. только он может гарантировать выполнение условия **_"генерирует данные каждые 0.5 сек вне зависимости от других компонентов программы"_**.


Описание классов задано в файле `basic_classes.py`. Основная логика реализована в файле `data_processing.py`. С подробным описанием логики работы можно без труда ознакомится в самом файле: по ходу выполенения расставлены комменты и описаны все докстринги + тайпинги, так что дублировать информацию здесь считаю излишним, опишу лишь некоторые моменты.


* Передача данных между классами Генератор и Обработчик реализована с помощью класса `multiprocessing.Queue`.
* Сигнал о завершении работы Генератора сохраняетя во флаге `multiprocessing.Event()`. Обработчик и Отправитель завершают свою работу после того, как поднят этот флаг.
* В консоль выводится дополнительная информация о работе: когда стартовал и закончил работу каждый из классов.


### Возможные улучшения

*Есть ряд улучшений/неточностей/уязвимостей, над которыми можно поработать:*

* Завершение работы процессов: работу класса Отправитель стоит заканчивать строго после окончания работы класса Обработчик. Сейчас оба этих класса заканчивают работу после окончания работы класса Генератор.
* Можно реализовать опциональный выбор способа хранения данных - pickle/json - т.к. файлы json можно открывать в текстовом формате и читать их "вручную".
* Из предыдущего пункта вытекает ещё один - валидация аргументов. Например, кроме наличия файла хранения данных (необязательный аргумент `--storage`) можно проверять, что он имеет расширение `.json` или `.pickle`.
* Так же можно сделать опциональной очистку данных из хранилища: сначала проверять, что они упешно отправились на сервер. Если нет - то попробовать отправить их ещё раз. Если не получается - оставить данные в хранилище и попробовать отправить со следующим батчем (в классе реализована задержка между отправками данных).